
use device: cuda
Model(
  (conv1): Conv2d(3, 128, kernel_size=(5, 5), stride=(1, 1))
  (bn1): BatchNorm2d()
  (relu1): ReLU()
  (dropout1): Dropout()
  (maxp1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1))
  (bn2): BatchNorm2d()
  (relu2): ReLU()
  (dropout2): Dropout()
  (maxp2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (fc): Linear(in_features=3200, out_features=10, bias=True)
  (loss): CrossEntropyLoss()
)
Epoch 1 of 40 took 183.52636218070984s
  learning rate:                 0.001
  training loss:                 1.4444573195278645
  training accuracy:             0.4983499879948795
  validation loss:               1.621533682346344
  validation accuracy:           0.3850999903678894
  best epoch:                    1
  best validation accuracy:      0.3850999903678894
  test loss:                     1.6163961398601532
  test accuracy:                 0.38929999083280564
Epoch 2 of 40 took 185.943115234375s
  learning rate:                 0.001
  training loss:                 1.0948974363505841
  training accuracy:             0.6192749869823456
  validation loss:               1.3652250504493713
  validation accuracy:           0.5346999877691269
  best epoch:                    2
  best validation accuracy:      0.5346999877691269
  test loss:                     1.3571817350387574
  test accuracy:                 0.5397999864816666
Epoch 3 of 40 took 184.3525562286377s
  learning rate:                 0.001
  training loss:                 0.9440100395679474
  training accuracy:             0.6717749851942062
  validation loss:               1.2432218790054321
  validation accuracy:           0.5716999879479409
  best epoch:                    3
  best validation accuracy:      0.5716999879479409
  test loss:                     1.2391823387145997
  test accuracy:                 0.5728999865055084
Epoch 4 of 40 took 191.57672476768494s
  learning rate:                 0.001
  training loss:                 0.8543713334202766
  training accuracy:             0.7033249835669995
  validation loss:               1.0725616025924682
  validation accuracy:           0.6660999822616577
  best epoch:                    4
  best validation accuracy:      0.6660999822616577
  test loss:                     1.0745467752218247
  test accuracy:                 0.6682999855279923
Epoch 5 of 40 took 192.62929892539978s
  learning rate:                 0.001
  training loss:                 0.7813424022495746
  training accuracy:             0.7290249826014041
  validation loss:               1.1329782336950303
  validation accuracy:           0.6361999836564064
  best epoch:                    4
  best validation accuracy:      0.6660999822616577
  test loss:                     1.0745467752218247
  test accuracy:                 0.6682999855279923
Epoch 6 of 40 took 199.61358523368835s
  learning rate:                 0.001
  training loss:                 0.7332604773342609
  training accuracy:             0.7462249818444252
  validation loss:               1.024285358786583
  validation accuracy:           0.6753999823331833
  best epoch:                    6
  best validation accuracy:      0.6753999823331833
  test loss:                     1.025645424723625
  test accuracy:                 0.6765999847650528
Epoch 7 of 40 took 194.01309657096863s
  learning rate:                 0.001
  training loss:                 0.6892905289679766
  training accuracy:             0.7611499813199043
  validation loss:               1.0275720077753068
  validation accuracy:           0.6751999831199647
  best epoch:                    6
  best validation accuracy:      0.6753999823331833
  test loss:                     1.025645424723625
  test accuracy:                 0.6765999847650528
Epoch 8 of 40 took 187.8734941482544s
  learning rate:                 0.001
  training loss:                 0.6572660453617573
  training accuracy:             0.772274982482195
  validation loss:               0.9115885585546494
  validation accuracy:           0.7324999815225601
  best epoch:                    8
  best validation accuracy:      0.7324999815225601
  test loss:                     0.9205524677038193
  test accuracy:                 0.7247999823093414
Epoch 9 of 40 took 192.05759572982788s
  learning rate:                 0.001
  training loss:                 0.6165208319574594
  training accuracy:             0.7863999789953232
  validation loss:               0.9770228374004364
  validation accuracy:           0.6935999852418899
  best epoch:                    8
  best validation accuracy:      0.7324999815225601
  test loss:                     0.9205524677038193
  test accuracy:                 0.7247999823093414
Epoch 10 of 40 took 185.18013834953308s
  learning rate:                 0.001
  training loss:                 0.5958735732734204
  training accuracy:             0.790699979364872
  validation loss:               0.8747182255983352
  validation accuracy:           0.7494999808073044
  best epoch:                    10
  best validation accuracy:      0.7494999808073044
  test loss:                     0.8767249029874802
  test accuracy:                 0.741599982380867
Traceback (most recent call last):
  File "/home/chen/ANN/2023-HW2/codes/cnn/main.py", line 137, in <module>
    train_acc, train_loss = train_epoch(mlp_model, X_train, y_train, optimizer)
  File "/home/chen/ANN/2023-HW2/codes/cnn/main.py", line 65, in train_epoch
    loss_, acc_ = model(X_batch, y_batch)
  File "/home/chen/anaconda3/envs/d2l/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/chen/ANN/2023-HW2/codes/cnn/model.py", line 97, in forward
    y_hat = self.dropout2(y_hat)
  File "/home/chen/anaconda3/envs/d2l/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/chen/ANN/2023-HW2/codes/cnn/model.py", line 50, in forward
    mask = torch.bernoulli(torch.full(input.shape, self.p))
KeyboardInterrupt